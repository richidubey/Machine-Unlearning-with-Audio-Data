{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93072ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b63a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/general/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ba94b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker ID: 59\n",
      "Audio path: 7_59_29.wav\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "# Load dataset normally\n",
    "dataset = load_dataset(\"gilkeyio/AudioMNIST\", streaming=False)\n",
    "\n",
    "# Disable decoding for the audio column\n",
    "dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "# Now you can safely access speaker_id without decoding audio\n",
    "sample = dataset['train'][0]\n",
    "print(\"Speaker ID:\", sample['speaker_id'])\n",
    "\n",
    "# Audio file path is still accessible\n",
    "print(\"Audio path:\", sample['audio']['path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da818c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "# Ensure audio gets decoded\n",
    "dataset = dataset.cast_column(\"audio\", Audio(decode=True))\n",
    "\n",
    "# Now you can access the array directly\n",
    "sample = dataset['train'][0]\n",
    "signal = sample['audio']['array']      # NumPy array\n",
    "fs = sample['audio']['sampling_rate']  # int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801b0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speaker_id', 'audio', 'digit', 'gender', 'accent', 'age', 'native_speaker', 'origin'],\n",
      "        num_rows: 24000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['speaker_id', 'audio', 'digit', 'gender', 'accent', 'age', 'native_speaker', 'origin'],\n",
      "        num_rows: 6000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/dpjvn_z10m74g_59wc45hqb40000gp/T/ipykernel_40124/1871771416.py:1: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted embedding!\n",
      "Shape of the voiceprint: torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "encoder = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\", \n",
    "    savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    ")\n",
    "\n",
    "sample = dataset['train'][0]\n",
    "signal = torch.tensor(sample['audio']['array'], dtype=torch.float32).unsqueeze(0)  # shape: (1, num_samples)\n",
    "fs = sample['audio']['sampling_rate']\n",
    "\n",
    "# The model expects audio at 16kHz, so resample if necessary\n",
    "if fs != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)\n",
    "    signal = resampler(signal)\n",
    "\n",
    "# 3. Get the embedding\n",
    "with torch.no_grad():\n",
    "    embedding = encoder.encode_batch(signal) # Shape will be (1, 1, 192)\n",
    "\n",
    "# Squeeze to get the final vector\n",
    "embedding = embedding.squeeze() # Shape is now (192)\n",
    "\n",
    "print(\"Successfully extracted embedding!\")\n",
    "print(\"Shape of the voiceprint:\", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using smaller dataset for pipeline test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [03:04<00:00,  5.42 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:38<00:00,  5.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted embeddings!\n",
      "{'speaker_id': '59', 'digit': 7, 'gender': 1, 'accent': 'German', 'age': 31, 'native_speaker': False, 'origin': 'Europe, Germany, Berlin', 'embedding': [-2.6190550327301025, 8.382065773010254, -27.288331985473633, 3.5866665840148926, 15.723471641540527, -32.97261047363281, 17.482685089111328, 27.093141555786133, 3.7568836212158203, -21.41496467590332, -5.328612327575684, 34.56608581542969, 2.608301877975464, -43.041954040527344, -3.2688894271850586, -2.453723669052124, 12.625601768493652, -21.025436401367188, 22.610679626464844, 1.498557686805725, -34.95357131958008, -3.0614869594573975, 25.82297134399414, 11.781750679016113, 6.656280517578125, 31.23275375366211, -14.543136596679688, 20.62252426147461, 42.4605712890625, -12.683794975280762, 14.99742317199707, 2.4680941104888916, -22.08287811279297, 15.312729835510254, -3.7390244007110596, 10.76298713684082, 11.025298118591309, 7.638171195983887, -10.915263175964355, -4.514418601989746, -2.1196634769439697, -24.223621368408203, 51.11465835571289, -2.7967369556427, -17.982885360717773, -31.849815368652344, 50.56789016723633, 7.279767036437988, -3.6182825565338135, -1.9223556518554688, -17.49945068359375, -16.639720916748047, 3.386173725128174, -45.5321044921875, -11.234989166259766, 12.12729549407959, -6.3963775634765625, 11.420336723327637, -28.110300064086914, 9.52175235748291, 5.646851539611816, -4.482022762298584, 8.805593490600586, 5.006515026092529, 4.374205589294434, -4.774260997772217, -6.52529239654541, 29.473615646362305, 24.078960418701172, 6.27686882019043, -5.178161144256592, -6.174732208251953, 7.619649410247803, -5.6472368240356445, -37.61820602416992, 12.518692016601562, 26.806703567504883, -34.84099197387695, 33.592811584472656, 31.16704559326172, -32.823097229003906, -1.5357953310012817, 20.431468963623047, 9.71183967590332, 31.36602020263672, 37.25307083129883, -20.763399124145508, 9.987523078918457, -14.732239723205566, -2.11336612701416, 37.339046478271484, 24.429174423217773, -28.035240173339844, -6.513177871704102, -42.19654083251953, 23.27467155456543, -7.049670696258545, 27.9039306640625, -8.300884246826172, 11.937264442443848, -28.59575080871582, 15.65207576751709, -36.55615997314453, -51.37904357910156, -2.9500515460968018, 47.79396057128906, 33.81645202636719, 9.005125999450684, 1.7481716871261597, 18.5728702545166, 84.91252136230469, 24.619115829467773, 11.604888916015625, -3.5103847980499268, -25.651594161987305, -33.93242645263672, 41.02394104003906, -1.8458245992660522, -15.663954734802246, -42.33608627319336, 34.20699691772461, 37.771183013916016, 18.224071502685547, 12.33260440826416, 38.52246856689453, -6.375510215759277, -9.073264122009277, -21.6751766204834, 15.690511703491211, -26.788898468017578, 16.234905242919922, 8.84750747680664, 14.898740768432617, -14.190470695495605, 6.265419960021973, -11.050480842590332, 25.737165451049805, -6.868588924407959, 29.98999786376953, -64.67072296142578, 34.52882385253906, -30.46459197998047, -5.083177089691162, 23.220802307128906, -11.382147789001465, 4.957310199737549, -4.622216701507568, -6.721199035644531, -24.10255241394043, -48.270198822021484, 22.075708389282227, -18.33533477783203, -16.805269241333008, -10.288893699645996, 4.107199668884277, 11.790020942687988, 13.192301750183105, 44.52933120727539, -18.253637313842773, 35.9012336730957, -19.071659088134766, -36.036746978759766, -7.825558185577393, -1.865683674812317, -27.628250122070312, -26.268430709838867, 5.711217403411865, 35.099388122558594, -10.879740715026855, -22.30140495300293, -5.638103485107422, 20.635150909423828, -40.15702438354492, -13.12928295135498, 8.611042976379395, -48.16337585449219, 54.03639221191406, 18.510257720947266, 6.838582992553711, -22.617645263671875, 10.520745277404785, -34.36524963378906, 19.965576171875, 9.369046211242676, -12.07906723022461, 17.889745712280273, 8.68441104888916, 2.9532997608184814, -15.725643157958984, 15.293530464172363, -33.87352752685547, 21.215322494506836]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "# 1. Load the dataset normally\n",
    "full_dataset = load_dataset(\"gilkeyio/AudioMNIST\")\n",
    "\n",
    "small_tr_dataset = full_dataset['train'].select(range(1000))\n",
    "small_te_dataset = full_dataset['test'].select(range(200))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': small_tr_dataset,\n",
    "    'test': small_te_dataset\n",
    "})\n",
    "\n",
    "print(\"Using smaller dataset for pipeline test\")\n",
    "\n",
    "# 2. Load the pre-trained encoder\n",
    "encoder = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    ")\n",
    "\n",
    "# 3. Define the CORRECTED extraction function\n",
    "def extract_embedding(batch):\n",
    "    \"\"\"\n",
    "    Function to extract embeddings from a batch of DECODED audio data.\n",
    "    `batch['audio']` is a list of audio dictionaries.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    # `batch['audio']` is a LIST where each element is a dictionary.\n",
    "    # We need to iterate through this list.\n",
    "    for audio_data in batch['audio']:\n",
    "        # Add a check for potentially corrupt data points\n",
    "        if audio_data is None or audio_data['array'] is None:\n",
    "            continue\n",
    "\n",
    "        signal_array = audio_data['array']\n",
    "        fs = audio_data['sampling_rate']\n",
    "\n",
    "        # Convert numpy array to torch tensor\n",
    "        signal = torch.tensor(signal_array, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if fs != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)\n",
    "            signal = resampler(signal)\n",
    "\n",
    "        # Extract embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = encoder.encode_batch(signal)\n",
    "            embedding = embedding.squeeze()\n",
    "        embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "    return {\"embedding\": embeddings}\n",
    "\n",
    "\n",
    "# 4. Use map to apply the function to the entire dataset\n",
    "# This structure is now correct for the batching logic.\n",
    "dataset_with_embeddings = dataset.map(\n",
    "    extract_embedding,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=[\"audio\"] # We no longer need the raw audio, this saves memory\n",
    ")\n",
    "\n",
    "# Now your dataset has a new column 'embedding'\n",
    "print(\"Successfully extracted embeddings!\")\n",
    "print(dataset_with_embeddings['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c7dff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 80430.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 22287.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./audio_mnist_with_embeddings\"\n",
    "dataset_with_embeddings.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f3ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pre-processed dataset from disk!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speaker_id', 'digit', 'gender', 'accent', 'age', 'native_speaker', 'origin', 'embedding'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['speaker_id', 'digit', 'gender', 'accent', 'age', 'native_speaker', 'origin', 'embedding'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample from the loaded dataset:\n",
      "{'speaker_id': '59', 'digit': 7, 'gender': 1, 'accent': 'German', 'age': 31, 'native_speaker': False, 'origin': 'Europe, Germany, Berlin', 'embedding': [-2.6190550327301025, 8.382065773010254, -27.288331985473633, 3.5866665840148926, 15.723471641540527, -32.97261047363281, 17.482685089111328, 27.093141555786133, 3.7568836212158203, -21.41496467590332, -5.328612327575684, 34.56608581542969, 2.608301877975464, -43.041954040527344, -3.2688894271850586, -2.453723669052124, 12.625601768493652, -21.025436401367188, 22.610679626464844, 1.498557686805725, -34.95357131958008, -3.0614869594573975, 25.82297134399414, 11.781750679016113, 6.656280517578125, 31.23275375366211, -14.543136596679688, 20.62252426147461, 42.4605712890625, -12.683794975280762, 14.99742317199707, 2.4680941104888916, -22.08287811279297, 15.312729835510254, -3.7390244007110596, 10.76298713684082, 11.025298118591309, 7.638171195983887, -10.915263175964355, -4.514418601989746, -2.1196634769439697, -24.223621368408203, 51.11465835571289, -2.7967369556427, -17.982885360717773, -31.849815368652344, 50.56789016723633, 7.279767036437988, -3.6182825565338135, -1.9223556518554688, -17.49945068359375, -16.639720916748047, 3.386173725128174, -45.5321044921875, -11.234989166259766, 12.12729549407959, -6.3963775634765625, 11.420336723327637, -28.110300064086914, 9.52175235748291, 5.646851539611816, -4.482022762298584, 8.805593490600586, 5.006515026092529, 4.374205589294434, -4.774260997772217, -6.52529239654541, 29.473615646362305, 24.078960418701172, 6.27686882019043, -5.178161144256592, -6.174732208251953, 7.619649410247803, -5.6472368240356445, -37.61820602416992, 12.518692016601562, 26.806703567504883, -34.84099197387695, 33.592811584472656, 31.16704559326172, -32.823097229003906, -1.5357953310012817, 20.431468963623047, 9.71183967590332, 31.36602020263672, 37.25307083129883, -20.763399124145508, 9.987523078918457, -14.732239723205566, -2.11336612701416, 37.339046478271484, 24.429174423217773, -28.035240173339844, -6.513177871704102, -42.19654083251953, 23.27467155456543, -7.049670696258545, 27.9039306640625, -8.300884246826172, 11.937264442443848, -28.59575080871582, 15.65207576751709, -36.55615997314453, -51.37904357910156, -2.9500515460968018, 47.79396057128906, 33.81645202636719, 9.005125999450684, 1.7481716871261597, 18.5728702545166, 84.91252136230469, 24.619115829467773, 11.604888916015625, -3.5103847980499268, -25.651594161987305, -33.93242645263672, 41.02394104003906, -1.8458245992660522, -15.663954734802246, -42.33608627319336, 34.20699691772461, 37.771183013916016, 18.224071502685547, 12.33260440826416, 38.52246856689453, -6.375510215759277, -9.073264122009277, -21.6751766204834, 15.690511703491211, -26.788898468017578, 16.234905242919922, 8.84750747680664, 14.898740768432617, -14.190470695495605, 6.265419960021973, -11.050480842590332, 25.737165451049805, -6.868588924407959, 29.98999786376953, -64.67072296142578, 34.52882385253906, -30.46459197998047, -5.083177089691162, 23.220802307128906, -11.382147789001465, 4.957310199737549, -4.622216701507568, -6.721199035644531, -24.10255241394043, -48.270198822021484, 22.075708389282227, -18.33533477783203, -16.805269241333008, -10.288893699645996, 4.107199668884277, 11.790020942687988, 13.192301750183105, 44.52933120727539, -18.253637313842773, 35.9012336730957, -19.071659088134766, -36.036746978759766, -7.825558185577393, -1.865683674812317, -27.628250122070312, -26.268430709838867, 5.711217403411865, 35.099388122558594, -10.879740715026855, -22.30140495300293, -5.638103485107422, 20.635150909423828, -40.15702438354492, -13.12928295135498, 8.611042976379395, -48.16337585449219, 54.03639221191406, 18.510257720947266, 6.838582992553711, -22.617645263671875, 10.520745277404785, -34.36524963378906, 19.965576171875, 9.369046211242676, -12.07906723022461, 17.889745712280273, 8.68441104888916, 2.9532997608184814, -15.725643157958984, 15.293530464172363, -33.87352752685547, 21.215322494506836]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "load_path = \"./audio_mnist_with_embeddings\"\n",
    "processed_dataset = load_from_disk(load_path)\n",
    "\n",
    "print(\"Successfully loaded pre-processed dataset from disk!\")\n",
    "print(processed_dataset)\n",
    "print(\"\\nSample from the loaded dataset:\")\n",
    "print(processed_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41087aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n",
      "Number of unique speakers found: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "all_speaker_ids = np.concatenate([\n",
    "    dataset_with_embeddings['train']['speaker_id'], \n",
    "    dataset_with_embeddings['test']['speaker_id']\n",
    "])\n",
    "\n",
    "label_encoder.fit(all_speaker_ids)\n",
    "\n",
    "train_labels = label_encoder.transform(dataset_with_embeddings['train']['speaker_id'])\n",
    "test_labels = label_encoder.transform(dataset_with_embeddings['test']['speaker_id'])\n",
    "\n",
    "\n",
    "train_embeddings = torch.tensor(np.array(dataset_with_embeddings['train']['embedding']), dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels_tensor)\n",
    "\n",
    "test_embeddings = torch.tensor(np.array(dataset_with_embeddings['test']['embedding']), dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "print(f\"Number of unique speakers found: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4daa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SpeakerClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "input_dim = 192 # From the ECAPA model\n",
    "num_speakers = len(label_encoder.classes_)\n",
    "model = SpeakerClassifier(input_dim, num_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa852227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_speakers is  3\n"
     ]
    }
   ],
   "source": [
    "print(\"num_speakers is \", num_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "179b6578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0000\n",
      "Epoch [2/10], Loss: 0.0000\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.0000\n",
      "Epoch [7/10], Loss: 0.0000\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for embeddings, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e6becef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test embeddings: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for embeddings, labels in test_loader:\n",
    "        outputs = model(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test embeddings: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c06d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
